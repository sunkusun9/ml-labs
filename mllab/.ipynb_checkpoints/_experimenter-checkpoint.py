import re
import os
import uuid
import pickle as pkl
import shutil
import traceback
import warnings
from pathlib import Path

import pandas as pd

from sklearn.model_selection import ShuffleSplit

from ._data_wrapper import wrap, unwrap
from ._node import NodeGroup, Node, RootNode
from ._describer import desc_spec, desc_pipeline, desc_node, desc_node_vars
from ._metric import Metric
from ._stacking import Stacking
from ._logger import DefaultLogger

class Experimenter():
    def __init__(
            self, data, path, data_names = None, sp = ShuffleSplit(n_splits=1, random_state=1), sp_v=None, splitter_params=None, title=None, data_key=None,
            logger = DefaultLogger(level=['info', 'progress'])
        ):
        self.logger = logger
        self.path = Path(path)
        if not os.path.exists(path):
            self.path.mkdir(parents=True, exist_ok=True)
            self.logger.info(f"ğŸ“ Created directory: {self.path}")
        self.train_idx_list = list()
        self.valid_idx_list = list()
        data_native = data
        data = wrap(data)
        self.root = data

        # ì‹¤í—˜ íƒ€ì´í‹€ ì €ì¥
        self.title = title

        # data ì‹ë³„ì (load ì‹œ ê²€ì¦ìš©)
        self.data_key = data_key

        # splitter ì„¤ì • ì €ì¥
        self.sp = sp
        self.sp_v = sp_v
        self.splitter_params = splitter_params if splitter_params is not None else {}
        self.exp_id = str(uuid.uuid4())

        split_params = {}

        if data_names is None:
            data_names = data.get_columns()
        for k, v in self.splitter_params.items():
            split_params[k] = unwrap(data.select_columns(v))

        for train_idx, valid_idx in sp.split(data_native, **split_params):
            if sp_v is not None:
                train_data = data.iloc(train_idx)
                train_data_native = unwrap(train_data)

                inner_split_params = {'X': train_data_native}
                for k, v in self.splitter_params.items():
                    inner_split_params[k] = unwrap(train_data.select_columns(v))

                self.train_idx_list.append([
                    (train_idx[train_v_idx], train_idx[valid_v_idx])
                    for train_v_idx, valid_v_idx in sp_v.split(**inner_split_params)
                ])
            else:
                self.train_idx_list.append([
                    (train_idx, None)
                ])
            self.valid_idx_list.append(valid_idx)
        self.nodes = {None: RootNode(self, data)}
        self.grps = {}
        self.metric = {}
        self.stacking = {}

    @staticmethod
    def create(data, path, data_names=None, sp=ShuffleSplit(n_splits=1, random_state=1), sp_v=None, splitter_params=None, title=None, data_key=None,
            logger = DefaultLogger(level=['info', 'progress'])):
        
        if os.path.exists(path):
            raise RuntimeError(f"Exists: {self.path}")
        return Experimenter(
            data, path, data_names, sp=sp, sp_v=sp_v, splitter_params=splitter_params, title=title, data_key=data_key,
            logger = logger)

    def get_n_splits(self):
        return len(self.train_idx_list)

    def add_metric(self, name, target_edges, output_var, metric_func, include_train=False):
        """Metric ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•˜ì—¬ ì¶”ê°€

        Args:
            name: metric ì´ë¦„
            target_edges: íƒ€ê²Ÿ edges
            output_var: ì¶œë ¥ ë³€ìˆ˜
            metric_func: metric í•¨ìˆ˜
            include_train: train ê²°ê³¼ í¬í•¨ ì—¬ë¶€ (ê¸°ë³¸ê°’: False)

        Returns:
            Metric: ìƒì„±ëœ Metric ì¸ìŠ¤í„´ìŠ¤
        """
        # __metric í´ë” ìƒì„± (ìµœì´ˆ ì¶”ê°€ ì‹œ)
        metric_dir = self.path / "__metric"
        if not metric_dir.exists():
            metric_dir.mkdir(parents=True, exist_ok=True)

        metric = Metric(
            name=name,
            experimenter=self,
            target_edges=target_edges,
            output_var=output_var,
            metric_func=metric_func,
            include_train=include_train
        )
        self.metric[name] = metric
        self._save()
        return metric

    def add_stacking(self, name, target_edges, output_var, method='mean', include_target=True):
        """Stacking ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•˜ì—¬ ì¶”ê°€

        Args:
            name: stacking ì´ë¦„
            target_edges: íƒ€ê²Ÿ edges
            output_var: ì¶œë ¥ ë³€ìˆ˜
            method: ì§‘ê³„ ë°©ë²• (ê¸°ë³¸ê°’: 'mean')
            include_target: íƒ€ê²Ÿ í¬í•¨ ì—¬ë¶€ (ê¸°ë³¸ê°’: True)

        Returns:
            Stacking: ìƒì„±ëœ Stacking ì¸ìŠ¤í„´ìŠ¤
        """

        stacking = Stacking(
            experimenter=self,
            target_edges=target_edges,
            output_var=output_var,
            method=method,
            include_target=include_target
        )
        stacking.name = name
        stacking.save_config()
        self.stacking[name] = stacking
        self._save()
        return stacking

    def _validate_name(self, name):
        """Node ë˜ëŠ” NodeGroup ì´ë¦„ ê²€ì¦

        Args:
            name: ê²€ì¦í•  ì´ë¦„

        Raises:
            ValueError: ì´ë¦„ì´ ìœ íš¨í•˜ì§€ ì•Šì„ ê²½ìš°
        """
        if name is None:
            return

        # '__' í¬í•¨ ê¸ˆì§€
        if '__' in name:
            raise ValueError(f"Name '{name}' cannot contain '__'")

        # íŒŒì¼/í´ë”ëª…ìœ¼ë¡œ ì‚¬ìš© ë¶ˆê°€í•œ ë¬¸ì ê¸ˆì§€
        invalid_chars = ['/', '\\', '\0', '<', '>', ':', '"', '|', '?', '*']
        for char in invalid_chars:
            if char in name:
                raise ValueError(f"Name '{name}' cannot contain '{char}'")

    def _find_descendants(self, node_name):
        """íŠ¹ì • ë…¸ë“œì— ì˜ì¡´í•˜ëŠ” ëª¨ë“  í•˜ìœ„ ë…¸ë“œë“¤ì„ ì°¾ìŒ (BFS)

        output_edgesë¥¼ í™œìš©í•˜ì—¬ íš¨ìœ¨ì ìœ¼ë¡œ íƒìƒ‰
        """
        descendants = set()
        queue = [node_name]

        while queue:
            current = queue.pop(0)

            if current not in self.nodes:
                continue

            # output_edges: ì´ ë…¸ë“œë¥¼ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ë…¸ë“œ ì´ë¦„ ë¦¬ìŠ¤íŠ¸
            for child_name in self.nodes[current].output_edges:
                if child_name not in descendants:
                    descendants.add(child_name)
                    queue.append(child_name)

        return descendants

    def _check_cycle(self, node_name, new_edges):
        """íŠ¹ì • ë…¸ë“œì— ìƒˆë¡œìš´ edgesë¥¼ ì¶”ê°€í–ˆì„ ë•Œ ì‚¬ì´í´ì´ ìƒê¸°ëŠ”ì§€ ì²´í¬

        Args:
            node_name: ì²´í¬í•  ë…¸ë“œ ì´ë¦„
            new_edges: ì¶”ê°€í•  edges ë¦¬ìŠ¤íŠ¸ [(edge_name, var), ...]

        Returns:
            tuple: (has_cycle: bool, cycle_edges: list)
                - has_cycle: ì‚¬ì´í´ì´ ìˆìœ¼ë©´ True, ì—†ìœ¼ë©´ False
                - cycle_edges: ì‚¬ì´í´ì„ ë§Œë“œëŠ” edge ì´ë¦„ë“¤ ë¦¬ìŠ¤íŠ¸
        """
        # node_nameì˜ descendantsë¥¼ ë¨¼ì € êµ¬í•¨
        descendants = self._find_descendants(node_name)

        cycle_edges = []
        for edge_name, _ in new_edges:
            # Root(None)ë¡œì˜ edgeëŠ” ì‚¬ì´í´ì„ ë§Œë“¤ì§€ ì•ŠìŒ
            if edge_name is None:
                continue

            # edge_nameì´ ì‹¤ì œ ë…¸ë“œì¸ì§€ í™•ì¸
            if edge_name not in self.nodes:
                continue

            # edge_nameì´ node_nameì˜ descendantsì— ìˆìœ¼ë©´ ì‚¬ì´í´
            # node_name -> ... -> edge_name (ì´ë¯¸ ì¡´ì¬)
            # node_name -> edge_name (ìƒˆë¡œ ì¶”ê°€)
            # ì´ë©´ node_name -> edge_name -> ... -> node_name ì‚¬ì´í´ì´ ìƒê¹€
            if edge_name in descendants:
                cycle_edges.append(edge_name)

        if cycle_edges:
            return True, cycle_edges
        return False, []
    
    def _check_edges(self, edges):
        if edges is None:
            return False
        for name, _ in edges:
            if name is None:
                continue
            if name not in self.nodes:
                raise ValueError(f"Edge node '{name}' not found")
            if self.nodes[name].grp.role != 'pipe':
                raise ValueError(f"Edge node '{name}' must be a pipe node, got '{self.nodes[name].grp.role}'")
        return True

    def _get_all_nodes_in_grp(self, grp):
        """ê·¸ë£¹ê³¼ í•˜ìœ„ ê·¸ë£¹ì˜ ëª¨ë“  ë…¸ë“œ ì´ë¦„ì„ ìˆ˜ì§‘"""
        result = list(grp.nodes)
        for child_grp in grp.child_grps:
            result.extend(self._get_all_nodes_in_grp(child_grp))
        return result

    def _compute_node_edges(self, node_name, new_grp_edges=None):
        """ë…¸ë“œì˜ ìµœì¢… edges ê³„ì‚° (ê·¸ë£¹ ìƒì† í¬í•¨)

        Args:
            node_name: ë…¸ë“œ ì´ë¦„
            new_grp_edges: ìƒˆë¡œ ì ìš©í•  ê·¸ë£¹ edges (Noneì´ë©´ í˜„ì¬ ê·¸ë£¹ attrs ì‚¬ìš©)
        """
        if node_name not in self.nodes:
            return []

        node = self.nodes[node_name]
        node_own_edges = list(node.org_attr['edges']) if node.org_attr and node.org_attr['edges'] else []

        if new_grp_edges is not None:
            # ìƒˆ ê·¸ë£¹ edges + ë…¸ë“œ ìì²´ edges
            return new_grp_edges + node_own_edges
        else:
            # í˜„ì¬ ê·¸ë£¹ attrsì—ì„œ edges ê°€ì ¸ì˜¤ê¸°
            grp_attrs = node.grp.get_attrs() if node.grp else {}
            grp_edges = grp_attrs.get('edges', [])
            return grp_edges + node_own_edges

    def set_grp(self, name, role=None, processor=None, edges=[], X=None, y=None, method=None, parent_grp=None, adapter=None, params=None):
        self._validate_name(name)
        self._check_edges(edges)
        if name in self.nodes:
            raise ValueError(f"Name '{name}' already exists as a node")

        # parent_grpê°€ ë¬¸ìì—´ì´ë©´ grpsì—ì„œ ì°¾ê¸°
        if parent_grp is not None:
            if parent_grp not in self.grps:
                raise ValueError(f"Parent group '{parent_grp}' not found")
            parent_grp = self.grps.get(parent_grp)
            if role is None:
                role = parent_grp.role
        if role not in ['pipe', 'exp']:
            raise ValueError(f"Role must be 'pipe' or 'exp', got '{role}'")
        # 1. ìƒˆë¡œìš´ ê·¸ë£¹ì¼ ê²½ìš° ì¶”ê°€
        if name not in self.grps:
            self._check_edges(edges)
            # NodeGroup ìƒì„±
            grp = NodeGroup(self, name, role, processor=processor, edges=edges, X=X, y=y, method=method, parent_grp=parent_grp, adapter=adapter, params=params)

            # parentì˜ child_grpsì— ì¶”ê°€
            if parent_grp is not None:
                parent_grp.child_grps.append(grp)

            # grps ë”•ì…”ë„ˆë¦¬ì— ë“±ë¡
            self.grps[name] = grp

            # ë””ë ‰í„°ë¦¬ ìƒì„±
            if grp.path is not None and not grp.path.exists():
                grp.path.mkdir(parents=True, exist_ok=True)
            grp.save_info()
            self._save()
            return grp

        grp = self.grps[name]
        if grp.role != role:
            raise ValueError(f"Cannot change role of group '{name}': existing '{grp.role}', requested '{role}'")
        old_grp_path = grp.path
        # 3. edges ë³€ê²½ ì‹œ ìˆœí™˜ êµ¬ì¡° ì²´í¬ (ë³€ê²½ ì „ ê²€ì¦)
        if edges is not None:
            new_edges = edges if isinstance(edges, list) else [edges]

            # ì´ ê·¸ë£¹ê³¼ í•˜ìœ„ ê·¸ë£¹ì˜ ëª¨ë“  ë…¸ë“œ ìˆ˜ì§‘
            all_affected_nodes = self._get_all_nodes_in_grp(grp)

            # ê° ë…¸ë“œì— ëŒ€í•´ ìƒˆ edgesë¡œ ìˆœí™˜ êµ¬ì¡° ì²´í¬
            for node_name in all_affected_nodes:
                if node_name not in self.nodes:
                    continue

                node = self.nodes[node_name]
                # ë…¸ë“œì˜ ê·¸ë£¹ ê³„ì¸µì—ì„œ í˜„ì¬ grpì˜ ìœ„ì¹˜ë¥¼ ê³ ë ¤í•˜ì—¬ ìµœì¢… edges ê³„ì‚°
                # ë¶€ëª¨ ê·¸ë£¹ì˜ edges + ìƒˆ edges + ìì‹ ê·¸ë£¹ì˜ edges + ë…¸ë“œ ìì²´ edges
                node_own_edges = list(node.org_attr['edges']) if node.org_attr and node.org_attr['edges'] else []

                # ê·¸ë£¹ ê³„ì¸µì—ì„œ edges ìˆ˜ì§‘ (í˜„ì¬ grpëŠ” new_edgesë¡œ ëŒ€ì²´)
                grp_edges = []
                current_grp = node.grp
                while current_grp is not None:
                    if current_grp.name == name:
                        # ë³€ê²½ ëŒ€ìƒ ê·¸ë£¹: ìƒˆ edges ì‚¬ìš©
                        grp_edges = new_edges + grp_edges
                    else:
                        grp_edges = current_grp.edges + grp_edges
                    current_grp = current_grp.parent_grp

                final_edges = grp_edges + node_own_edges

                # ì‚¬ì´í´ ì²´í¬
                has_cycle, cycle_edges = self._check_cycle(node_name, final_edges)
                if has_cycle:
                    cycle_info = ", ".join([f"'{e}'" for e in cycle_edges])
                    raise ValueError(f"Cannot update group '{name}': node '{node_name}' would create cycle through edge(s) {cycle_info}")

        # 4. ê²€ì¦ í†µê³¼ - ì‹¤ì œ ë³€ê²½ ìˆ˜í–‰

        # parent_grp ë³€ê²½ ì²˜ë¦¬
        parent_changed = False
        new_parent = parent_grp
        if new_parent is not None and grp.parent_grp != new_parent:
            parent_changed = True
            # ì´ì „ parentì˜ child_grpsì—ì„œ ì œê±°
            if grp.parent_grp is not None:
                grp.parent_grp.child_grps.remove(grp)
            # ìƒˆë¡œìš´ parentì˜ child_grpsì— ì¶”ê°€
            grp.parent_grp = new_parent
            if new_parent is not None:
                new_parent.child_grps.append(grp)

        # ê·¸ë£¹ ì†ì„± ì—…ë°ì´íŠ¸
        if processor is not None:
            grp.processor = processor
        if edges is not None:
            grp.edges = edges if isinstance(edges, list) else [edges]
        if X is not None:
            grp.X = X
        if y is not None:
            grp.y = y
        if method is not None:
            grp.method = method
        if adapter is not None:
            grp.adapter = adapter
        if params is not None:
            grp.params.update(params)

        # parent ë³€ê²½ ì‹œ ë””ë ‰í„°ë¦¬ êµ¬ì¡° ì—…ë°ì´íŠ¸
        if parent_changed:
            self._ensure_grp_directories(grp)

        # 5. ì˜í–¥ë°›ëŠ” ë…¸ë“œë“¤ ì´ˆê¸°í™”
        all_affected_nodes = self._get_all_nodes_in_grp(grp)
        if len(all_affected_nodes) == 0:
            self.logger.info(f"Group '{name}' updated (no nodes to rebuild)")
            return grp
        
        node_to_initialize = self._get_effected_nodes(all_affected_nodes)
        for node in node_to_initialize:
            node.initialize()

        for v in self.metric.values():
            v.reset_nodes(node_to_initialize)
        
        for v in self.stacking.values():
            v.reset_nodes(node_to_initialize)

        new_grp_path = grp.path
        if old_grp_path != new_grp_path:
            os.makedirs(dst_dir, exist_ok=True)
            for name in os.listdir(old_grp_path):
                src_path = os.path.join(old_grp_path, name)
                dst_path = os.path.join(dst_dir, name)
                shutil.move(src_path, dst_path)
        grp.save_info()
        self.logger.info(f"Group '{name}' updated, {len(node_to_initialize)} node(s) affected")
        self._save()
        return grp

    def rename_grp(self, name_from, name_to):
        self._validate_name(name_to)

        if name_from not in self.grps:
            raise ValueError(f"Group '{name_from}' not found")
        if name_to in self.grps:
            raise ValueError(f"Group '{name_to}' already exists")
            
        grp = self.grps[name_from]
        old_grp_path = grp.path
        grp.name = name_to
        if grp.parent_grp is not None:
            # ì´ì „ parentì˜ child_grpsì—ì„œ ì œê±°
            if grp.parent_grp is not None:
                grp.parent_grp.child_grps.remove(name_from)
                grp.parent_grp.child_grps.append(name_to)
        new_grp_path = grp.path
        os.makedirs(new_grp_path, exist_ok=True)
        for name in os.listdir(old_grp_path):
            src_path = os.path.join(old_grp_path, name)
            dst_path = os.path.join(new_grp_path, name)
            shutil.move(src_path, dst_path)
        shutil.rmtree(old_grp_path)
        del self.grps[name_from]
        self.grps[name_to] = grp
        self._save()
        
    def _get_effected_nodes(self, nodes):
        # ìš°ì„ ìˆœìœ„ ì•Œê³ ë¦¬ì¦˜: BFSë¡œ ë…¸ë“œë“¤ì˜ ë¹Œë“œ ìš°ì„ ìˆœìœ„ ê²°ì •
        priorities = {}
        queue = []

        # ë³€ê²½ëœ ê·¸ë£¹ì˜ ë…¸ë“œë“¤ì„ Rootë¡œ ìš°ì„ ìˆœìœ„ 1 í• ë‹¹
        for node_name in nodes:
            priorities[node_name] = 1
            queue.append((node_name, 1))

        # BFSë¡œ í•˜ìœ„ ë…¸ë“œë“¤ íƒìƒ‰
        while queue:
            current_node, current_priority = queue.pop(0)

            # í˜„ì¬ ë…¸ë“œì— ì˜ì¡´í•˜ëŠ” í•˜ìœ„ ë…¸ë“œë“¤ ì°¾ê¸°
            descendants = self._find_descendants(current_node)

            for desc_node in descendants:
                new_priority = current_priority + 1
                # ê°€ì¥ ë§ˆì§€ë§‰ì— ë°°ì •ëœ ìš°ì„ ìˆœìœ„ê°€ ìµœì¢… ìš°ì„ ìˆœìœ„
                if desc_node not in priorities or priorities[desc_node] < new_priority:
                    priorities[desc_node] = new_priority
                    queue.append((desc_node, new_priority))
        # ìš°ì„ ìˆœìœ„ ìˆœìœ¼ë¡œ ì •ë ¬ (ë‚®ì€ ìˆ«ìê°€ ë¨¼ì €)
        sorted_nodes = sorted(priorities.items(), key=lambda x: x[1])
        return [self.nodes[i[0]] for i in sorted_nodes]
    
    def remove_grp(self, name):
        if name not in self.grps:
            raise ValueError(f"Group '{name}' not found")

        grp = self.grps[name]

        # child groupì´ ìˆìœ¼ë©´ ì œê±° ë¶ˆê°€
        if len(grp.child_grps) > 0:
            raise ValueError(f"Cannot remove group '{name}': has {len(grp.child_grps)} child group(s)")

        # ì†Œì† Nodeê°€ ìˆìœ¼ë©´ ì œê±° ë¶ˆê°€
        if len(grp.nodes) > 0:
            raise ValueError(f"Cannot remove group '{name}': has {len(grp.nodes)} node(s)")

        # parentì˜ child_grpsì—ì„œ ì œê±°
        if grp.parent_grp is not None:
            grp.parent_grp.child_grps.remove(grp)

        # grps ë”•ì…”ë„ˆë¦¬ì—ì„œ ì œê±°
        del self.grps[name]

        self.logger.info(f"Group '{name}' removed")
        self._save()

    def get_parents(self, node_name):
        if node_name not in self.nodes:
            return []

        node = self.nodes[node_name]
        if node.grp_name is None:
            return []

        result = []
        current_grp = self.grps.get(node.grp_name)

        while current_grp is not None:
            result.append(current_grp.name)
            current_grp = current_grp.parent_grp

        return result

    def get_node_names(self, query):
        if isinstance(query, str):
            if query not in self.grps:
                return []

            result = []
            def collect_nodes(grp):
                result.extend(grp.nodes)
                for child_grp in grp.child_grps:
                    collect_nodes(child_grp)

            collect_nodes(self.grps[query])
            return result

        elif isinstance(query, re.Pattern):
            return [name for name in self.nodes.keys() if name is not None and query.search(name)]

        else:
            raise ValueError(f"query must be str or re.Pattern, got {type(query)}")

    def remove_node(self, name):
        """ë…¸ë“œë¥¼ ì œê±°

        Args:
            name: ì œê±°í•  ë…¸ë“œ ì´ë¦„

        Raises:
            ValueError: ë…¸ë“œê°€ ì¡´ì¬í•˜ì§€ ì•Šê±°ë‚˜, ìì‹ ë…¸ë“œê°€ ìˆëŠ” ê²½ìš°
        """
        # ë…¸ë“œê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
        if name not in self.nodes:
            raise ValueError(f"Node '{name}' not found")

        # Root ë…¸ë“œëŠ” ì œê±° ë¶ˆê°€
        if name is None:
            raise ValueError("Cannot remove Root node")

        # ìì‹ ë…¸ë“œ(descendants)ê°€ ìˆëŠ”ì§€ í™•ì¸
        descendants = self._find_descendants(name)
        if descendants:
            descendants_list = sorted(descendants)
            raise ValueError(f"Cannot remove node '{name}': has {len(descendants)} dependent node(s): {descendants_list}")

        node = self.nodes[name]

        # output_edges ë¬´ê²°ì„± ìœ ì§€: ë¶€ëª¨ ë…¸ë“œë“¤ì˜ output_edgesì—ì„œ ì œê±°
        self._update_output_edges(name, node.edges, None)

        # ê·¸ë£¹ì— ì†í•´ìˆìœ¼ë©´ ê·¸ë£¹ì˜ nodes ë¦¬ìŠ¤íŠ¸ì—ì„œ ì œê±°
        grp_name = node.grp.name if node.grp is not None else None
        if grp_name is not None and grp_name in self.grps:
            grp = self.grps[grp_name]
            if name in grp.nodes:
                grp.nodes.remove(name)
                self.logger.info(f"Removed '{name}' from group '{grp_name}'")

        node.remove()
        # nodes ë”•ì…”ë„ˆë¦¬ì—ì„œ ì œê±°
        del self.nodes[name]

        self.logger.info(f"Node '{name}' removed")
        self._save()

    def finalize(self, nodes):
        if nodes is None:
            # ê¸°ì¡´ ë™ì‘: ëª¨ë“  root groupì˜ ë…¸ë“œ
            node_names = list(self.nodes.keys())
        elif isinstance(nodes, list):
            node_names = [n for n in nodes if n in self.nodes]
        elif isinstance(nodes, str):
            pat = re.compile(nodes)
            node_names = [k for k in self.nodes.keys() if k is not None and pat.search(k)]
        else:
            raise ValueError(f"nodes must be None, list, or str, got {type(nodes)}")
        target_nodes = list()
        for i in node_names:
            node = self.nodes[i]
            if type(node) != RootNode and node.grp.role == 'exp' and node.status == 'built':
                self.logger.info(f"Finalize '{i}'")
                node.finalize()

    def reinitialize(self, nodes):
        if nodes is None:
            # ê¸°ì¡´ ë™ì‘: ëª¨ë“  root groupì˜ ë…¸ë“œ
            node_names = list(self.nodes.keys())
        elif isinstance(nodes, list):
            node_names = [n for n in nodes if n in self.nodes]
        elif isinstance(nodes, str):
            pat = re.compile(nodes)
            node_names = [k for k in self.nodes.keys() if k is not None and pat.search(k)]
        else:
            raise ValueError(f"nodes must be None, list, or str, got {type(nodes)}")
        target_nodes = list()
        for i in node_names:
            node = self.nodes[i]
            if type(node) != RootNode and node.status == 'finalized':
                self.logger.info(f"reinitialize '{i}'")
                node.initialize()

    def close_exp(self):
        for k, node in self.nodes.items():
            if type(node) != RootNode and node.status == 'built':
                self.logger.info(f"Finalize '{k}'")
                node.finalize()
    
    def _update_output_edges(self, node_name, old_edges, new_edges):
        """output_edges ë¬´ê²°ì„± ìœ ì§€

        Args:
            node_name: í˜„ì¬ ë…¸ë“œ ì´ë¦„
            old_edges: ì´ì „ edges ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ ì œê±°ë§Œ ìŠ¤í‚µ)
            new_edges: ìƒˆ edges ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ ì¶”ê°€ë§Œ ìŠ¤í‚µ)
        """
        # ì´ì „ edgesì—ì„œ í˜„ì¬ ë…¸ë“œ ì œê±°
        if old_edges is not None:
            for edge_name, _ in old_edges:
                if edge_name in self.nodes:
                    parent_node = self.nodes[edge_name]
                    if node_name in parent_node.output_edges:
                        parent_node.output_edges.remove(node_name)

        # ìƒˆ edgesì— í˜„ì¬ ë…¸ë“œ ì¶”ê°€
        if new_edges is not None:
            for edge_name, _ in new_edges:
                if edge_name in self.nodes:
                    parent_node = self.nodes[edge_name]
                    if node_name not in parent_node.output_edges:
                        parent_node.output_edges.append(node_name)

    def set_node(
        self, name, grp, processor = None, edges = list(), X = None, y = None,
        method = None, adapter = 'default', params = None
    ):
        self._validate_name(name)

        if name in self.grps:
            raise ValueError(f"Name '{name}' already exists as a group")

        if grp not in self.grps:
            raise ValueError(f"Group '{grp}' not found")
        
        self._check_edges(edges)

        # ê¸°ì¡´ ë…¸ë“œê°€ ìˆëŠ”ì§€ í™•ì¸
        is_update = name in self.nodes
        old_edges = None
        old_output_edges = None
        if is_update:
            old_edges = self.nodes[name].edges
            old_output_edges = self.nodes[name].output_edges

        # params ê¸°ë³¸ê°’ ì²˜ë¦¬
        if params is None:
            params = {}

        # org_attr ìƒì„± (ì›ë³¸ íŒŒë¼ë¯¸í„° ì €ì¥)
        org_attr = {
            'processor': processor,
            'edges': edges,
            'X': X,
            'y': y,
            'method': method,
            'adapter': adapter,
            'params': params
        }

        # grp ì´ë¦„ ì €ì¥
        grp_name = grp
        grp_obj = self.grps.get(grp, None)
        if grp_obj is None:
            raise ValueError(f"Group '{grp}' not found")

        # grpì˜ attrsë¥¼ ê°€ì ¸ì™€ì„œ ê¸°ë³¸ê°’ìœ¼ë¡œ ì‚¬ìš©
        grp_attrs = grp_obj.get_attrs()

        # íŒŒë¼ë¯¸í„°ë¡œ ë„˜ì–´ì˜¨ ê°’ì´ Noneì´ ì•„ë‹ˆë©´ override
        if processor is None:
            processor = grp_attrs.get('processor', None)
        if len(grp_attrs['edges']) > 0:
            edges = edges + grp_attrs['edges']
        if X is None:
            X = grp_attrs['X']
        if y is None:
            y = grp_attrs['y']
        if method is None:
            method = grp_attrs.get('method', None)
        if adapter is None:
            adapter = grp_attrs.get('adapter', None)

        # paramsëŠ” grpì˜ paramsë¥¼ ê°€ì ¸ì™€ì„œ í˜„ì¬ paramsë¡œ override
        merged_params = {**grp_attrs['params'], **params}

        # processor ì²´í¬
        if processor is None:
            raise ValueError(f"Cannot create node '{name}': processor is required")

        # methodê°€ Noneì´ë©´ ê¸°ë³¸ê°’ ì„¤ì •
        if method is None:
            raise ValueError(f"Cannot create node '{name}': method is required")

        # edgesë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ì •ê·œí™”
        if not isinstance(edges, list):
            edges = [edges]

        # ì‚¬ì´í´ ì²´í¬
        has_cycle, cycle_edges = self._check_cycle(name, edges)
        if has_cycle:
            cycle_info = ", ".join([f"'{e}'" for e in cycle_edges])
            raise ValueError(f"Cannot add node '{name}': would create cycle through edge(s) {cycle_info}")

        # output_edges ë¬´ê²°ì„± ì—…ë°ì´íŠ¸
        self._update_output_edges(name, old_edges, edges)

        node = Node(self, name, processor, edges, X = X, y = y, method = method, grp = grp_obj, adapter = adapter, org_attr = org_attr, params = merged_params)
        if old_output_edges is not None:
            node.output_edges = old_output_edges
        # grpì— ë…¸ë“œ ì¶”ê°€
        if grp_obj is not None:
            if name not in grp_obj.nodes:
                grp_obj.nodes.append(name)

        # ê¸°ì¡´ ë…¸ë“œë¥¼ ì—…ë°ì´íŠ¸í•œ ê²½ìš°, í•˜ìœ„ ë…¸ë“œë“¤ë„ ì¬ë¹Œë“œ
        if is_update:
            descendants = self._find_descendants(name)
            if descendants:
                self.logger.info(f"Effected {len(descendants)} dependent node(s): {sorted(descendants)}")
                for i in descendants:
                    self.nodes[i].initialize()

                for v in self.metric.values():
                    v.reset_nodes(descendants)
                
                for v in self.stacking.values():
                    v.reset_nodes(descendants)

        # ê·¸ë£¹ì´ ë³€ê²½ëœ ê²½ìš° ì´ì „ ê·¸ë£¹ì—ì„œ ë…¸ë“œ ì œê±°
        if is_update and self.nodes[name].grp.name != grp_name:
            old_grp_name = self.nodes[name].grp.name
            if old_grp_name is not None and old_grp_name in self.grps:
                old_grp = self.grps[old_grp_name]
                if name in old_grp.nodes:
                    old_grp.nodes.remove(name)
                    self.logger.info(f"Removed '{name}' from group '{old_grp_name}'")
            if grp_name is not None:
                self.logger.info(f"Moved '{name}' to group '{grp_name}'")

        self.nodes[name] = node
        self._save()
        return node

    def build(self, nodes = None, rebuild = False):
        if nodes is None:
            # ê¸°ì¡´ ë™ì‘: ëª¨ë“  root groupì˜ ë…¸ë“œ
            node_names = list(self.nodes.keys())
        elif isinstance(nodes, list):
            node_names = [n for n in nodes if n in self.nodes]
        elif isinstance(nodes, str):
            pat = re.compile(nodes)
            node_names = [k for k in self.nodes.keys() if k is not None and pat.search(k)]
        else:
            raise ValueError(f"nodes must be None, list, or str, got {type(nodes)}")
        target_nodes = [
            i for i in self._get_effected_nodes([None]) if type(i) != RootNode and i.grp.role == 'pipe' and (i.name in node_names and (i.status is None or rebuild))
        ]
        self.logger.info(f"Building {len(target_nodes)} node(s)")
        for node in target_nodes:
            node.start_build()
        n_splits = self.get_n_splits()
        self.logger.start_progress("Build", n_splits)
        try:
            for i in range(n_splits):
                self.logger.update_progress(i)
                self.logger.start_progress("Node", len(target_nodes))
                for ni, node in enumerate(target_nodes):
                    self.logger.update_progress(ni)
                    self.logger._progress[-1][0] = node.name
                    with warnings.catch_warnings(record=True) as caught:
                        warnings.simplefilter("always")
                        node.build_idx(i)
                        for w in caught:
                            self.logger.warning(f"[{node.name}] fold {i}: {w.category.__name__}: {w.message}")
                self.logger.end_progress(len(target_nodes))
            self.logger.end_progress(n_splits)
        except Exception as e:
            self.logger.clear_progress()
            self.logger.info(f"Build failed at fold {i}, node '{node.name}': {type(e).__name__}: {e}")
            self.logger.info(traceback.format_exc())
            raise
        for node in target_nodes:
            node.end_build()
        self.logger.info(f"Build complete: {len(target_nodes)} node(s)")
    
    def exp(self, nodes = None):
        if nodes is None:
            # ê¸°ì¡´ ë™ì‘: ëª¨ë“  root groupì˜ ë…¸ë“œ
            node_names = list(self.nodes.keys())
        elif isinstance(nodes, list):
            node_names = [n for n in nodes if n in self.nodes]
        elif isinstance(nodes, str):
            pat = re.compile(nodes)
            node_names = [k for k in self.nodes.keys() if k is not None and pat.search(k)]
        else:
            raise ValueError(f"nodes must be None, list, or str, got {type(nodes)}")
        target_nodes = [
            i for i in self._get_effected_nodes([None]) if type(i) != RootNode and i.grp.role == 'exp' and (i.name in node_names and i.status is None)
        ]
        self.logger.info(f"Experimenting {len(target_nodes)} node(s)")

        # start_experiment for all nodes
        for node in target_nodes:
            node.start_experiment()

        # _start for metrics and stackings
        for v in self.metric.values():
            for node in target_nodes:
                v._start(node.name)
        for v in self.stacking.values():
            for node in target_nodes:
                v._start(node.name)

        # experiment loop
        n_splits = self.get_n_splits()
        self.logger.start_progress("Exp", n_splits)
        try:
            for i in range(n_splits):
                self.logger.update_progress(i)
                # prepare target metrics data
                target_metrics = {
                    k: v._get_data(i) for k, v in self.metric.items()
                }

                self.logger.start_progress("Node", len(target_nodes))
                for ni, node in enumerate(target_nodes):
                    self.logger.update_progress(ni)
                    self.logger._progress[-1][0] = node.name
                    with warnings.catch_warnings(record=True) as caught:
                        warnings.simplefilter("always")
                        result_iter = node.experiment(i)

                        stacks = {k: list() for k in self.stacking.keys()}
                        sub_metrics = {k: list() for k in self.metric.keys()}

                        for n, result_data in enumerate(result_iter):
                            # collect metrics
                            for k, v in self.metric.items():
                                sub_metric = v._get_metric(target_metrics[k][n], result_data)
                                sub_metric = {k_sub: v_sub for k_sub, v_sub in sub_metric.items()}
                                sub_metrics[k].append(sub_metric)
                            # collect stacking data
                            for k, v in self.stacking.items():
                                _valid = v._get_valid(result_data)
                                if _valid is not None:
                                    stacks[k].append(_valid)

                        for w in caught:
                            self.logger.warning(f"[{node.name}] fold {i}: {w.category.__name__}: {w.message}")

                    # set metrics
                    for k, v in self.metric.items():
                        v._set_metric(node.name, i, sub_metrics[k])
                    # aggregate and stack
                    for k, v in self.stacking.items():
                        if len(stacks[k]) > 0:
                            stk = v._aggregate(iter(stacks[k]))
                            v._stack(node.name, i, stk)
                self.logger.end_progress(len(target_nodes))
            self.logger.end_progress(n_splits)
        except Exception as e:
            self.logger.clear_progress()
            self.logger.info(f"Exp failed at fold {i}, node '{node.name}': {type(e).__name__}: {e}")
            self.logger.info(traceback.format_exc())
            raise

        # end_experiment for all nodes
        for node in target_nodes:
            node.end_experiment()

        # _end for metrics and stackings
        for v in self.metric.values():
            for node in target_nodes:
                v._end(node.name)
        for v in self.stacking.values():
            for node in target_nodes:
                v._end(node.name)

        self.logger.info(f"Experimentation complete: {len(target_nodes)} node(s)")

    def get_data(self, idx, edges):
        def ret_data_func(data_list):
            for z in zip(*data_list):
                train_sub, valid_sub, outer_valid_sub = list(), list(), list()
                for (train_data, train_v_data), outer_valid_data in z:
                    train_sub.append(train_data)
                    if train_v_data is not None:
                        valid_sub.append(train_v_data)
                    outer_valid_sub.append(outer_valid_data)

                train_concat = type(train_sub[0]).concat(train_sub, axis=1)
                outer_concat = type(outer_valid_sub[0]).concat(outer_valid_sub, axis=1)
                if len(valid_sub) > 0:
                    valid_concat = type(valid_sub[0]).concat(valid_sub, axis=1)
                    yield (train_concat, valid_concat), outer_concat
                else:
                    yield (train_concat, None), outer_concat

        data_list = list()
        for node_name, var in edges:
            data_list.append(self.nodes[node_name].get_data(idx, var))
        return ret_data_func(data_list)
    
    def get_data_train(self, idx, edges):
        def ret_data_func(data_list):
            for z in zip(*data_list):
                train_sub, valid_sub = list(), list()
                for train_data, train_v_data in z:
                    train_sub.append(train_data)
                    if train_v_data is not None:
                        valid_sub.append(train_v_data)
                train_concat = type(train_sub[0]).concat(train_sub, axis=1)
                if len(valid_sub) > 0:
                    valid_concat = type(valid_sub[0]).concat(valid_sub, axis=1)
                    yield train_concat, valid_concat
                else:
                    yield train_concat, None

        data_list = list()
        for node_name, var in edges:
            data_list.append(self.nodes[node_name].get_data_train(idx, var))
        return ret_data_func(data_list)
    
    def get_data_valid(self, idx, edges):
        """ì™¸ë¶€ ê²€ì¦ ë°ì´í„°ì— ëŒ€í•œ ì²˜ë¦¬ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜´

        Args:
            idx: outer fold ì¸ë±ìŠ¤
            edges: [(node_name, var), ...] í˜•íƒœì˜ edge ë¦¬ìŠ¤íŠ¸

        Yields:
            valid_concat: ê° inner fold ëª¨ë¸ë¡œ ì²˜ë¦¬ëœ ì™¸ë¶€ ê²€ì¦ ë°ì´í„° ê²°ê³¼ (concat)
        """
        def ret_data_func(data_list):
            for z in zip(*data_list):
                outer_valid_sub = list()
                for outer_valid_data in z:
                    outer_valid_sub.append(outer_valid_data)

                # DataWrapperì˜ concat ì‚¬ìš©
                outer_concat = type(outer_valid_sub[0]).concat(outer_valid_sub, axis=1)
                yield outer_concat

        data_list = list()
        for node_name, var in edges:
            data_list.append(self.nodes[node_name].get_data_valid(idx, var))
        return ret_data_func(data_list)

    def split(self, edges):
        for idx in range(len(self.train_idx_list)):
            yield self.get_data(idx, edges)
    
    def get_node_output(self, idx, node, var = None):
        if node not in self.nodes:
            raise ValueError(f"Node '{node}' not found")
        return self.nodes[node].get_data(idx, var)

    def get_node_train_output(self, idx, node, var=None):
        if node not in self.nodes:
            raise ValueError(f"Node '{node}' not found")
        return self.nodes[node].get_data_train(idx, var)

    def get_node_valid_output(self, idx, node, var=None):
        if node not in self.nodes:
            raise ValueError(f"Node '{node}' not found")
        return self.nodes[node].get_data_valid(idx, var)

    def get_node_info(self):
        lines = [f"# Experiment Pipeline Summary\n"]
        lines.append(f"- **Root**: {type(self.root).__name__}\n")

        for name, node in self.nodes.items():
            if name is None:
                continue
            processor_name = node.processor.__name__
            edges_info = ", ".join([
                f"{n or 'Root'}{f'[{v}]' if v else ''}"
                for n, v in node.edges
            ])
            lines.append(f"## {name}")
            lines.append(f"- **Processor**: {processor_name}")
            lines.append(f"- **Method**: {node.method}")
            lines.append(f"- **Edges**: {edges_info}")

            descendants = self._find_descendants(name)
            if descendants:
                lines.append(f"- **Descendants**: {sorted(descendants)}")
            lines.append("")

        return "\n".join(lines)

    def desc_spec(self):
        """ì‹¤í—˜ ìŠ¤í™ì„ Markdownìœ¼ë¡œ ë°˜í™˜"""
        return desc_spec(self)

    def desc_pipeline(self, max_depth=None, direction='TD'):
        """íŒŒì´í”„ë¼ì¸ êµ¬ì¡°ë¥¼ Mermaid Markdownìœ¼ë¡œ ë°˜í™˜

        Args:
            max_depth: ìµœëŒ€ í‘œì‹œ ê¹Šì´ (Noneì´ë©´ ë¬´ì œí•œ)
            direction: ê·¸ë˜í”„ ë°©í–¥ ('TD': Top-Down, 'LR': Left-Right)
        """
        return desc_pipeline(self, max_depth, direction)

    def desc_node(self, node_name, direction='TD', show_params=False):
        """íŠ¹ì • ë…¸ë“œê¹Œì§€ì˜ ì—°ê²° êµ¬ì¡°ë¥¼ Mermaid Markdownìœ¼ë¡œ ë°˜í™˜

        Args:
            node_name: ëŒ€ìƒ ë…¸ë“œ ì´ë¦„
            direction: ê·¸ë˜í”„ ë°©í–¥ ('TD': Top-Down, 'LR': Left-Right)
            show_params: Trueì´ë©´ ë…¸ë“œì˜ íŒŒë¼ë¯¸í„° ì •ë³´ë¥¼ í‘œì‹œ (default: False)
        """
        return desc_node(self, node_name, direction, show_params)

    def desc_node_vars(self, node_name, idx):
        """íŠ¹ì • ë…¸ë“œì˜ ì…ë ¥/ì¶œë ¥ ë³€ìˆ˜ë¥¼ ë¶„ì„

        Args:
            node_name: ëŒ€ìƒ ë…¸ë“œ ì´ë¦„
            idx: ì™¸ë¶€ fold ì¸ë±ìŠ¤

        Returns:
            list: [(ì…ë ¥ë³€ìˆ˜ ë¦¬ìŠ¤íŠ¸, ì¶œë ¥ë³€ìˆ˜ ë¦¬ìŠ¤íŠ¸, í•´ë‹¹ ë‚´ë¶€ í´ë“œ index ë¦¬ìŠ¤íŠ¸), ...]
                  ë“±ì¥ ë¹ˆë„ì˜ ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ì •ë ¬
        """
        return desc_node_vars(self, node_name, idx)

    def get_node_vars(self, node_name, idx):
        """íŠ¹ì • ë…¸ë“œì˜ ì…ë ¥/ì¶œë ¥ ë³€ìˆ˜ë¥¼ ê°€ì ¸ì˜´

        Args:
            node_name: ëŒ€ìƒ ë…¸ë“œ ì´ë¦„
            idx: ì™¸ë¶€ fold ì¸ë±ìŠ¤

        Returns:
            list: [(ì…ë ¥ë³€ìˆ˜ ë¦¬ìŠ¤íŠ¸, ì¶œë ¥ë³€ìˆ˜ ë¦¬ìŠ¤íŠ¸, í•´ë‹¹ ë‚´ë¶€ í´ë“œ index ë¦¬ìŠ¤íŠ¸), ...]
                  ë“±ì¥ ë¹ˆë„ì˜ ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ì •ë ¬
        """
        if node_name not in self.nodes or node_name is None:
            raise ValueError(f"Node '{node_name}' not found")

        node = self.nodes[node_name]

        # ë…¸ë“œê°€ ë¹Œë“œë˜ì§€ ì•Šì•˜ìœ¼ë©´ ì—ëŸ¬
        if node.status != 'built':
            raise ValueError(f"Node '{node_name}' status should be built")

        # ì™¸ë¶€ foldì˜ ë‚´ë¶€ foldë“¤: [(processor, train_v, info), ...]
        inner_folds = node.get_exp_obj(idx)

        # (ì…ë ¥ë³€ìˆ˜ íŠœí”Œ, ì¶œë ¥ë³€ìˆ˜ íŠœí”Œ) -> ë‚´ë¶€ fold index ë¦¬ìŠ¤íŠ¸
        var_map = {}

        for inner_idx, (processor, train_v, info) in enumerate(inner_folds):
            # ì…ë ¥ ë³€ìˆ˜ì™€ ì¶œë ¥ ë³€ìˆ˜ ê°€ì ¸ì˜¤ê¸°
            input_vars = tuple(processor.X_) if hasattr(processor, 'X_') and processor.X_ is not None else ()
            output_vars = tuple(processor.output_vars) if hasattr(processor, 'output_vars') and processor.output_vars is not None else ()

            # íŠœí”Œ í‚¤ ìƒì„±
            key = (input_vars, output_vars)

            if key not in var_map:
                var_map[key] = []
            var_map[key].append(inner_idx)

        # ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ ìƒì„±: [(ì…ë ¥ë³€ìˆ˜ ë¦¬ìŠ¤íŠ¸, ì¶œë ¥ë³€ìˆ˜ ë¦¬ìŠ¤íŠ¸, ë‚´ë¶€ í´ë“œ index ë¦¬ìŠ¤íŠ¸), ...]
        result = []
        for (input_vars, output_vars), fold_indices in var_map.items():
            result.append((list(input_vars), list(output_vars), fold_indices))

        # ë“±ì¥ ë¹ˆë„(ë‚´ë¶€ í´ë“œ ê°œìˆ˜)ì˜ ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ì •ë ¬
        result.sort(key=lambda x: len(x[2]), reverse=True)

        return result

    def _get_grp_load_order(self):
        """NodeGroup ë¡œë”© ìˆœì„œë¥¼ BFSë¡œ ê³„ì‚° (parentê°€ ì—†ëŠ” ê·¸ë£¹ë¶€í„° ì‹œì‘)

        Returns:
            list: ë¡œë”© ìˆœì„œì— ë§ëŠ” (grp_name, parent_grp_name) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
        """
        result = []
        # ìµœìƒìœ„ ê·¸ë£¹ ì°¾ê¸° (parent_grpê°€ Noneì¸ ê·¸ë£¹)
        queue = [(grp.name, None) for grp in self.grps.values() if grp.parent_grp is None]

        while queue:
            grp_name, parent_name = queue.pop(0)
            result.append((grp_name, parent_name))
            grp = self.grps[grp_name]
            # ìì‹ ê·¸ë£¹ì„ íì— ì¶”ê°€
            for child_grp in grp.child_grps:
                queue.append((child_grp.name, grp_name))

        return result

    def _get_node_load_order(self):
        """Node ë¡œë”© ìˆœì„œë¥¼ ê³„ì‚° (_get_effected_nodes ì‚¬ìš©)

        Returns:
            list: ë¡œë”© ìˆœì„œì— ë§ëŠ” (node_name, grp_name) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
        """
        effected = self._get_effected_nodes([None])
        result = []
        for node in effected:
            if type(node) != RootNode:
                result.append((node.name, node.grp.name))
        return result

    def _save(self, filepath=None):
        """Experimenter ê°ì²´ë¥¼ íŒŒì¼ë¡œ ì €ì¥

        Args:
            filepath: ì €ì¥í•  íŒŒì¼ ê²½ë¡œ (Noneì´ë©´ self.path / '__exp.pkl')
        """
        if filepath is None:
            filepath = self.path / '__exp.pkl'

        # ì €ì¥í•  ë°ì´í„° êµ¬ì„± (dataëŠ” ì €ì¥í•˜ì§€ ì•ŠìŒ)
        save_data = {
            'data_key': self.data_key,
            'title': self.title,
            'sp': self.sp,
            'sp_v': self.sp_v,
            'splitter_params': self.splitter_params,
            'exp_id': self.exp_id,
            'grp_load_order': self._get_grp_load_order(),
            'node_load_order': self._get_node_load_order(),
            'metric_keys': list(self.metric.keys()),
            'stacking_keys': list(self.stacking.keys())
        }

        # print(f"ğŸ’¾ Saving Experimenter to {filepath}...")
        with open(filepath, 'wb') as f:
            pkl.dump(save_data, f)
        """
        print(f"âœ… Experimenter saved successfully")
        print(f"   - {len(save_data['node_load_order'])} node(s)")
        print(f"   - {len(save_data['grp_load_order'])} group(s)")
        print(f"   - {len(save_data['metric_keys'])} metric(s)")
        print(f"   - {len(save_data['stacking_keys'])} stacking(s)")
        """

    @staticmethod
    def load(filepath, data, data_key=None):
        """íŒŒì¼ì—ì„œ Experimenter ê°ì²´ë¥¼ ë¶ˆëŸ¬ì˜´

        Args:
            filepath: ë¶ˆëŸ¬ì˜¬ íŒŒì¼ ê²½ë¡œ
            data: ì‹¤í—˜ì— ì‚¬ìš©í•  ë°ì´í„°
            data_key: ë°ì´í„° ì‹ë³„ì (ì €ì¥ëœ data_keyì™€ ë¹„êµí•˜ì—¬ ê²€ì¦)

        Returns:
            Experimenter: ë¶ˆëŸ¬ì˜¨ Experimenter ê°ì²´

        Raises:
            ValueError: ì €ì¥ëœ data_keyì™€ ì „ë‹¬ëœ data_keyê°€ ì¼ì¹˜í•˜ì§€ ì•ŠëŠ” ê²½ìš°
        """
        from ._metric import Metric
        from ._stacking import Stacking

        filepath = Path(filepath)
        with open(filepath / '__exp.pkl', 'rb') as f:
            save_data = pkl.load(f)

        # data_key ê²€ì¦ (ì €ì¥ëœ data_keyê°€ Noneì´ ì•„ë‹Œ ê²½ìš°ì—ë§Œ)
        saved_data_key = save_data.get('data_key')
        if saved_data_key is not None and saved_data_key != data_key:
            raise ValueError(
                f"data_key mismatch: saved='{saved_data_key}', provided='{data_key}'"
            )

        # Experimenter ìƒì„±ì í™œìš©
        exp = Experimenter(
            data=data,
            path=filepath,
            sp=save_data['sp'],
            sp_v=save_data['sp_v'],
            splitter_params=save_data['splitter_params'],
            title=save_data['title'],
            data_key=saved_data_key
        )
        # exp_idë¥¼ ì €ì¥ëœ ê°’ìœ¼ë¡œ ë³µì›
        exp.exp_id = save_data['exp_id']

        # NodeGroup ë³µì› (ë¡œë”© ìˆœì„œëŒ€ë¡œ)
        for grp_name, parent_grp_name in save_data['grp_load_order']:
            parent_grp = exp.grps.get(parent_grp_name) if parent_grp_name else None
            grp = NodeGroup.load(exp, grp_name, parent_grp)
            exp.grps[grp_name] = grp

        # Node ë³µì› (ë¡œë”© ìˆœì„œëŒ€ë¡œ)
        for node_name, grp_name in save_data['node_load_order']:
            grp = exp.grps[grp_name]
            node = Node.load(exp, grp, node_name)
            exp.nodes[node_name] = node

        # output_edges ì¬êµ¬ì„±
        for node_name, node in exp.nodes.items():
            if node_name is None:
                continue
            for edge_name, _ in node.edges:
                if edge_name in exp.nodes:
                    parent_node = exp.nodes[edge_name]
                    if node_name not in parent_node.output_edges:
                        parent_node.output_edges.append(node_name)

        # Metric ë³µì›
        for metric_name in save_data['metric_keys']:
            metric = Metric.load_from_file(exp, metric_name)
            exp.metric[metric_name] = metric

        # Stacking ë³µì›
        for stacking_name in save_data['stacking_keys']:
            stacking = Stacking.load_from_file(exp, stacking_name)
            exp.stacking[stacking_name] = stacking

        exp.logger.info(f"Loaded: {len(exp.nodes) - 1} node(s), {len(exp.grps)} group(s), {len(exp.train_idx_list)} fold(s)")

        return exp

    def get_result(self, node, idx, result, params = {}):
        return self.nodes[node].get_result(idx, result, params)

    def get_results(self, node, result, params = {}):
        for i in range(self.get_n_splits()):
            yield list(
                self.get_result(node, i, result, params)
            )

    def get_results_agg(self, node, result, params = {}, agg_inner = True, agg_outer = True):
        if agg_outer and not agg_inner:
            raise ValueError("agg_outer requires agg_inner to be True")
        if not self.nodes[node].adapter_.result_objs[result][1]:
            raise ValueError(f"Result '{result}' is not mergeable across folds")
        l = list()
        for no, i in enumerate(self.get_results(node, result, params)):
            l.append(pd.concat([j.rename(no_i) for no_i, j in enumerate(i)], axis = 1).stack().rename(no))
        df = pd.concat(l, axis=1)
        if agg_inner:
            df = df.groupby(level=[i for i in range(len(df.index.levels) - 1)]).mean()
            if agg_outer:
                return df.mean(axis=1)
        return df

def create_like(exp, data, path, data_names=None, sp=None, sp_v=None, splitter_params=None, title=None, data_key=None):
    """ê¸°ì¡´ Experimenterì˜ êµ¬ì¡°ë¥¼ ë³µì œí•˜ì—¬ ìƒˆë¡œìš´ Experimenter ìƒì„±

    Args:
        exp: êµ¬ì¡°ë¥¼ ë³µì œí•  ì›ë³¸ Experimenter
        data: ìƒˆë¡œìš´ ë°ì´í„°
        path: ì‘ì—… ë””ë ‰í† ë¦¬ ê²½ë¡œ
        data_names: ìƒˆë¡œìš´ ë°ì´í„°ì˜ ì»¬ëŸ¼ëª… (Noneì´ë©´ ìë™)
        sp: ì™¸ë¶€ splitter (Noneì´ë©´ ì›ë³¸ê³¼ ë™ì¼)
        sp_v: ë‚´ë¶€ splitter (Noneì´ë©´ ì›ë³¸ê³¼ ë™ì¼, "remove"ì´ë©´ ì œê±°)
        splitter_params: splitterì— ì „ë‹¬í•  íŒŒë¼ë¯¸í„° (Noneì´ë©´ ì›ë³¸ê³¼ ë™ì¼)
        title: ì‹¤í—˜ íƒ€ì´í‹€ (Noneì´ë©´ ì›ë³¸ê³¼ ë™ì¼)
        data_key: ë°ì´í„° ì‹ë³„ì (Noneì´ë©´ ì›ë³¸ê³¼ ë™ì¼)

    Returns:
        Experimenter: ìƒˆë¡œ ìƒì„±ëœ Experimenter ì¸ìŠ¤í„´ìŠ¤
    """
    exp.logger.info("Creating new Experimenter with same structure...")

    if sp is None:
        sp = exp.sp
    if sp_v is None:
        sp_v = exp.sp_v
    elif sp_v == "remove":
        sp_v = None
    if splitter_params is None:
        splitter_params = exp.splitter_params.copy() if exp.splitter_params else None
    if title is None:
        title = exp.title
    if data_key is None:
        data_key = exp.data_key

    # ìƒˆ Experimenter ìƒì„±
    new_exp = Experimenter(
        data,
        path=path,
        data_names=data_names,
        sp=sp,
        sp_v=sp_v,
        splitter_params=splitter_params,
        title=title,
        data_key=data_key
    )
    new_exp.logger.info(f"Created base Experimenter with {len(new_exp.train_idx_list)} fold(s)")

    # ê·¸ë£¹ ë³µì œ (ë¶€ëª¨-ìì‹ ê´€ê³„ë¥¼ ìœ ì§€í•˜ê¸° ìœ„í•´ ìœ„ìƒ ì •ë ¬)
    # 1. ìµœìƒìœ„ ê·¸ë£¹ë¶€í„° BFSë¡œ ë³µì œ
    grp_mapping = {}  # ì›ë³¸ ê·¸ë£¹ëª… -> ìƒˆ ê·¸ë£¹ ê°ì²´

    # ìµœìƒìœ„ ê·¸ë£¹ ì°¾ê¸° (parent_grpê°€ Noneì¸ ê·¸ë£¹)
    top_level_grps = [grp for grp in exp.grps.values() if grp.parent_grp is None]

    def clone_group_recursive(orig_grp, parent_grp_name=None):
        """ê·¸ë£¹ì„ ì¬ê·€ì ìœ¼ë¡œ ë³µì œ"""
        new_grp = new_exp.set_grp(
            name=orig_grp.name,
            role=orig_grp.role,
            processor=orig_grp.processor,
            edges=orig_grp.edges[:],  # ë¦¬ìŠ¤íŠ¸ ë³µì‚¬
            X=orig_grp.X,
            y=orig_grp.y,
            method=orig_grp.method,
            parent_grp=parent_grp_name,
            adapter=orig_grp.adapter,
            params=orig_grp.params.copy() if orig_grp.params else {}
        )
        grp_mapping[orig_grp.name] = new_grp

        # ìì‹ ê·¸ë£¹ë“¤ë„ ë³µì œ
        for child_grp in orig_grp.child_grps:
            clone_group_recursive(child_grp, parent_grp_name=orig_grp.name)

    # ìµœìƒìœ„ ê·¸ë£¹ë¶€í„° ì¬ê·€ì ìœ¼ë¡œ ë³µì œ
    for grp in top_level_grps:
        clone_group_recursive(grp)

    new_exp.logger.info(f"Cloned {len(exp.grps)} group(s)")

    # ë…¸ë“œ ë³µì œ (ìœ„ìƒ ì •ë ¬: Rootë¶€í„° BFS)
    # 1. ë…¸ë“œì˜ ìš°ì„ ìˆœìœ„ ê³„ì‚° (BFS)
    node_priorities = {}
    queue = [(None, 0)]  # Rootë¶€í„° ì‹œì‘

    while queue:
        current_node, priority = queue.pop(0)

        if current_node in node_priorities:
            continue

        node_priorities[current_node] = priority

        # current_nodeë¥¼ edgeë¡œ ì°¸ì¡°í•˜ëŠ” child ë…¸ë“œë“¤ ì°¾ê¸°
        for name in current_node.output_edges :
            if name is not None and name not in node_priorities:
                queue.append((name, priority + 1))
                break

    # ìš°ì„ ìˆœìœ„ ìˆœìœ¼ë¡œ ë…¸ë“œ ì •ë ¬ (Root ì œì™¸)
    sorted_nodes = sorted(
        [(name, node) for name, node in exp.nodes.items() if name is not None],
        key=lambda x: node_priorities.get(x[0], float('inf'))
    )

    # ë…¸ë“œ ë³µì œ
    for name, orig_node in sorted_nodes:
        if orig_node.org_attr is not None:
            org = orig_node.org_attr
            new_exp.set_node(
                name,
                grp=orig_node.grp.name,
                processor=org['processor'],
                edges=org['edges'][:] if isinstance(org['edges'], list) else [org['edges']] if org['edges'] else [],
                X=org['X'],
                y=org['y'],
                method=org['method'],
                adapter=org['adapter'],
                params=org['params'].copy() if org['params'] else {}
            )

    new_exp.logger.info(f"Structure cloning complete: {len(exp.grps)} group(s), {len(sorted_nodes)} node(s)")

    return new_exp